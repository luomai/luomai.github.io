<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | Luo Mai</title>
    <link>https://luomai.github.io/post/</link>
      <atom:link href="https://luomai.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Fri, 20 Feb 2026 13:35:37 +0100</lastBuildDate>
    <image>
      <url>https://luomai.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url>
      <title>Posts</title>
      <link>https://luomai.github.io/post/</link>
    </image>
    
    <item>
      <title>[02/26, Paper] ContextPilot accepted to MLSys 2026.</title>
      <link>https://luomai.github.io/post/26-contxtpilot-mlsys/</link>
      <pubDate>Fri, 20 Feb 2026 13:35:37 +0100</pubDate>
      <guid>https://luomai.github.io/post/26-contxtpilot-mlsys/</guid>
      <description>&lt;p&gt;Excited to share that our paper &lt;strong&gt;ContextPilot&lt;/strong&gt; has been accepted to MLSys 2026! üéâ&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The Challenge:&lt;/strong&gt; As LLMs handle increasingly long contexts (think RAG, agent memory, multi-turn conversations), prefill latency becomes the bottleneck. Current acceleration methods face a tough trade-off: preserve reasoning quality OR improve efficiency‚Äîbut not both.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Our Solution:&lt;/strong&gt; ContextPilot introduces &lt;em&gt;context reuse&lt;/em&gt; as a new mechanism for faster long-context inference. The system intelligently identifies and reuses overlapping context blocks across different LLM interactions, maximizing KV-cache efficiency without sacrificing quality.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The Results:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Up to &lt;strong&gt;3√ó faster&lt;/strong&gt; prefill compared to state-of-the-art methods&lt;/li&gt;
&lt;li&gt;Quality preserved (or even improved!) at longer context lengths&lt;/li&gt;
&lt;li&gt;Modular architecture that integrates with existing inference engines&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Open-sourced and ready to use: 
&lt;a href=&#34;https://github.com/EfficientContext/ContextPilot&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/EfficientContext/ContextPilot&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Looking forward to presenting this work at MLSys 2026!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>[11/25, Paper] BitDecoding accepted to HPCA 2026.</title>
      <link>https://luomai.github.io/post/26-bitdecoding-hpca/</link>
      <pubDate>Tue, 25 Nov 2025 13:35:37 +0100</pubDate>
      <guid>https://luomai.github.io/post/26-bitdecoding-hpca/</guid>
      <description>&lt;p&gt;Thrilled to announce that our paper &lt;strong&gt;BitDecoding&lt;/strong&gt; has been accepted to HPCA 2026! üöÄ&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The Problem:&lt;/strong&gt; Long-context LLMs are memory-hungry. While low-bit KV-cache quantization (2-bit, 4-bit) can dramatically reduce memory footprint, existing systems are painfully slow‚Äîthey only use CUDA cores and completely ignore Tensor Cores, the GPU&amp;rsquo;s main computational powerhouse.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Our Innovation:&lt;/strong&gt; BitDecoding unlocks Tensor Cores for low-bit KV-cache inference. By cooperatively leveraging both CUDA cores and Tensor Cores with optimized layouts and smart parallelization, we achieve the best of both worlds: small memory footprint AND blazing-fast decoding.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The Impact:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Up to 8.9√ó faster&lt;/strong&gt; than FP16 FlashDecoding on Hopper and Blackwell&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;4.3√ó faster&lt;/strong&gt; than state-of-the-art low-bit system&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;3√ó lower latency&lt;/strong&gt; on LLaMA-3.1-8B with 128K context&lt;/li&gt;
&lt;li&gt;Works across Blackwell, Hopper and Ampere GPUs&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This opens the door for efficient long-context inference at scale. Looking forward to presenting at HPCA 2026!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>[09/25, Paper] MoE-CAP accepted to NeurIPS 2025 (Dataset and Benchmark Track).</title>
      <link>https://luomai.github.io/post/25-moecap-neurips/</link>
      <pubDate>Thu, 25 Sep 2025 13:35:37 +0100</pubDate>
      <guid>https://luomai.github.io/post/25-moecap-neurips/</guid>
      <description>&lt;p&gt;Excited to share that &lt;strong&gt;MoE-CAP&lt;/strong&gt; has been accepted to NeurIPS 2025 (Datasets and Benchmarks Track)! üéâ&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The Problem:&lt;/strong&gt; Sparse Mixture-of-Experts (MoE) models are the go-to architecture for scaling LLMs efficiently‚Äîbut deploying them is hard. They depend on heterogeneous compute and memory, making three-way trade-offs between &lt;strong&gt;Cost&lt;/strong&gt;, &lt;strong&gt;Accuracy&lt;/strong&gt;, and &lt;strong&gt;Performance (CAP)&lt;/strong&gt; inevitable. Yet no benchmark captures these trade-offs well.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Our Contribution:&lt;/strong&gt; MoE-CAP is the first benchmark purpose-built for MoE systems. Our key finding: achieving an optimal balance across all three CAP dimensions is fundamentally difficult‚ÄîMoE systems typically optimize two at the expense of the third. We call this the &lt;strong&gt;MoE-CAP trade-off&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What we introduce:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;üìä &lt;strong&gt;CAP Radar Diagram&lt;/strong&gt; ‚Äî a new visualization tool to intuitively map Cost-Accuracy-Performance trade-offs&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;S-MBU&lt;/strong&gt; (Sparse Memory Bandwidth Utilization) ‚Äî a sparsity-aware memory metric&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;S-MFU&lt;/strong&gt; (Sparse Model FLOPS Utilization) ‚Äî a sparsity-aware compute metric&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These metrics enable accurate, apples-to-apples benchmarking of MoE systems across diverse hardware and deployment scenarios.&lt;/p&gt;
&lt;p&gt;Paper: 
&lt;a href=&#34;https://arxiv.org/abs/2412.07067&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/abs/2412.07067&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Looking forward to presenting at NeurIPS 2025!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>[08/25, Grant] Win a prestigious award to build systems for powering AI4Math breakthroughs.</title>
      <link>https://luomai.github.io/post/25-ai4math/</link>
      <pubDate>Mon, 25 Aug 2025 13:35:37 +0100</pubDate>
      <guid>https://luomai.github.io/post/25-ai4math/</guid>
      <description>&lt;p&gt;Excited to share that our SketchPad project has been selected by the AI4Math fund to advance breakthroughs in mathematics! I‚Äôll contribute my expertise in building large-scale, efficient, and reliable AI systems. The project is led by Wenda Li, together with Huajian Xin, Lawrence C. Paulson, and me.&lt;/p&gt;
&lt;p&gt;Thanks to this generous support, we will be hiring several fully funded PhD students and Postdoctoral researchers. If you are passionate about pushing AI to tackle high-impact mathematical problems and other game-changing scientific discovery challenges, we‚Äôd love to hear from you.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>[07/25, Paper] WaferLLM, the world fastest LLM inference system, accepted to OSDI 2025.</title>
      <link>https://luomai.github.io/post/25-waferllm-osdi/</link>
      <pubDate>Tue, 01 Jul 2025 13:35:37 +0100</pubDate>
      <guid>https://luomai.github.io/post/25-waferllm-osdi/</guid>
      <description>&lt;p&gt;Thrilled to announce that &lt;strong&gt;WaferLLM&lt;/strong&gt;, the world&amp;rsquo;s fastest LLM inference system, has been accepted to OSDI 2025! üéâ&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The Opportunity:&lt;/strong&gt; Wafer-scale accelerators pack hundreds of thousands of AI cores with massive on-chip memory (tens of GB) and incredible bandwidth (tens of PB/s). But current LLM systems, built for GPUs, can&amp;rsquo;t harness this power‚Äîleaving most of the hardware idle.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Our Breakthrough:&lt;/strong&gt; WaferLLM is the &lt;em&gt;first&lt;/em&gt; LLM inference system purpose-built for wafer-scale architectures. We introduce:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Novel PLMR model capturing wafer-scale hardware characteristics&lt;/li&gt;
&lt;li&gt;Wafer-scale LLM parallelism across hundreds of thousands of cores&lt;/li&gt;
&lt;li&gt;MeshGEMM &amp;amp; MeshGEMV‚Äîthe first scalable implementations for wafer architectures&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;The Results:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;200√ó higher&lt;/strong&gt; accelerator utilization vs. state-of-the-art&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;606√ó faster&lt;/strong&gt; GEMV operations than NVIDIA A100&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;16√ó more energy-efficient&lt;/strong&gt; than A100&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;10-20√ó speedups&lt;/strong&gt; for full LLM inference vs. A100 clusters (SGLang, vLLM)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This opens a new frontier for LLM inference at unprecedented scale and efficiency. Open-sourced at: 
&lt;a href=&#34;https://github.com/MeshInfra/WaferLLM&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/MeshInfra/WaferLLM&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Looking forward to presenting at OSDI 2025!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>[03/25, Achievement] Starting August 2025, I‚Äôll be Reader (Associate Professor).</title>
      <link>https://luomai.github.io/post/25-reader-promotion/</link>
      <pubDate>Tue, 25 Mar 2025 13:35:37 +0100</pubDate>
      <guid>https://luomai.github.io/post/25-reader-promotion/</guid>
      <description>&lt;p&gt;After nearly five years at Edinburgh, I‚Äôm delighted to share that my promotion has been approved, and I will take up the role of Reader (Associate Professor) in August 2025. Sincere thanks to all my students and postdocs, my collaborators around the world, and my colleagues at the School of Informatics, EPCC, and the University for their support along the way.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>[10/24, Grant] Secured a prestigious ARIA grant with Imperial College &amp; Cambridge University.</title>
      <link>https://luomai.github.io/post/24-aria-award/</link>
      <pubDate>Sat, 26 Oct 2024 13:35:37 +0100</pubDate>
      <guid>https://luomai.github.io/post/24-aria-award/</guid>
      <description>&lt;p&gt;I am deeply honored to receive a prestigious grant from the Advanced Research + Invention Agency (ARIA) to develop a scalable and modular performance simulation framework for emerging AI models, systems, and hardware. This project will be co-led by Imperial (Aaron Zhao), Edinburgh (Luo Mai), and Cambridge (Robert Mullins), bringing together over 10 world-leading experts across the entire system stack. With this ¬£4.5M funding, we will build a critical mass of talent and skills to incubate game-changing AI systems, enabling a 1000X improvement in AI&amp;rsquo;s efficiency.&lt;/p&gt;
&lt;p&gt;See the 
&lt;a href=&#34;https://www.aria.org.uk/scaling-compute/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;official announcement&lt;/a&gt; from ARIA.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>[10/24, Paper] Tenplex, the first elastic LLM system, accepted to SOSP 2024.</title>
      <link>https://luomai.github.io/post/24-tenplex-sosp/</link>
      <pubDate>Fri, 25 Oct 2024 13:35:37 +0100</pubDate>
      <guid>https://luomai.github.io/post/24-tenplex-sosp/</guid>
      <description>&lt;p&gt;Tenplex is a new system enabling elastic training of LLMs with advanced multi-dimensional parallelism. The Tenplex paper has been accepted for presentation at the 30th Symposium on Operating Systems Principles (SOSP‚Äô24). We are currently preparing to open-source the project. Stay tuned.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>[07/24, Student Achievement] Congrats to Yao Fu on Winning 2024 Rising Star in ML &amp; Systems.</title>
      <link>https://luomai.github.io/post/24-risingstar-yao/</link>
      <pubDate>Sat, 10 Aug 2024 13:35:37 +0100</pubDate>
      <guid>https://luomai.github.io/post/24-risingstar-yao/</guid>
      <description>&lt;p&gt;This year‚Äôs Rising Stars cohort includes 41 junior researchers from 33 institutions globally. Selected from over 170 applicants, Yao is the sole recipient from the UK and one of three from Europe, the other two European students coming from ETH, Zurich.&lt;/p&gt;
&lt;p&gt;The 2024 Rising Start workshop was held in the NVIDIA HQ in Santa Clara, CA in July, where Yao presented his research.&lt;/p&gt;
&lt;p&gt;Yao is a third-year PhD student in Computer Science at The University of Edinburgh, supervised by Dr Luo Mai. His research lies at the intersection of machine learning and systems, focusing on performance and affordability. Recently, his work has centred on efficient serving systems for large language models, leading to two main projects: ServerlessLLM and the MoESys Leaderboard.&lt;/p&gt;
&lt;p&gt;See the coverage from 
&lt;a href=&#34;https://informatics.ed.ac.uk/news-events/news/latest-news/informatics-student-among-machine-learning-and-systems-rising-stars&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;our school&lt;/a&gt; and the 
&lt;a href=&#34;https://mlcommons.org/2024/06/2024-mlc-rising-stars/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;annoucement from MLCommon&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>[07/24, Paper] ServerlessLLM, the first serverless LLM system, accepted to OSDI 2024.</title>
      <link>https://luomai.github.io/post/24-serverlessllm-osdi/</link>
      <pubDate>Wed, 10 Jul 2024 13:35:37 +0100</pubDate>
      <guid>https://luomai.github.io/post/24-serverlessllm-osdi/</guid>
      <description>&lt;p&gt;ServerlessLLM is a new system enabling cost-effective serverless inference for LLMs by implementing a scalable and high-performance ‚Äúcheckpoint storage layer‚Äù on GPU servers. It achieves this through an innovative LLM checkpoint format, a multi-tier checkpoint loading subsystem, an efficient live migration algorithm, and a locality-friendly GPU serverless architecture.&lt;/p&gt;
&lt;p&gt;The ServerlessLLM paper has been accepted to the top systems conference, OSDI‚Äô24, and we are preparing to open-source the project. Stay tuned.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>[05/24, Award] Win a Microsoft Research Startrack Scholar Award.</title>
      <link>https://luomai.github.io/post/24-startrack-microsoft/</link>
      <pubDate>Fri, 10 May 2024 13:35:37 +0100</pubDate>
      <guid>https://luomai.github.io/post/24-startrack-microsoft/</guid>
      <description>&lt;p&gt;I am thrilled to receive the Microsoft Research Startrack Scholar Award, a prestigious honor for young faculty in the field of computer science.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>[03/24, Grant] Secured funds from EPSRC and industry partners to build a CDT for ML Systems.</title>
      <link>https://luomai.github.io/post/24-mlsyscdt-award/</link>
      <pubDate>Sat, 20 Jan 2024 13:35:37 +0100</pubDate>
      <guid>https://luomai.github.io/post/24-mlsyscdt-award/</guid>
      <description>&lt;p&gt;I am deeply thrilled to have secured an EPSRC grant (¬£8.7M) matched with ¬£9.2M from industry partners to establish a Centre for Doctoral Training (CDT) in ML Systems. This grant is co-led by Amos Storkey (PI), Mike O&amp;rsquo;Boyle (co-I), Ajitha Rajan (co-I), and myself (co-I). With this CDT, we are one of the few places in the world capable of creating the critical mass of talent, skills, and resources needed to incubate game-changing technologies for ML systems.&lt;/p&gt;
&lt;p&gt;Through this CDT, PhD students will have the opportunity to build expertise across the entire AI system stack‚Äîspanning models, algorithms, software, hardware, and large-scale clusters. The CDT is supported by UK EPSRC and over 20 industry partners, with that number still growing.&lt;/p&gt;
&lt;p&gt;See the 
&lt;a href=&#34;https://www.ukri.org/what-we-do/developing-people-and-skills/epsrc/studentships/centres-for-doctoral-training/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;announcement 1&lt;/a&gt; and 
&lt;a href=&#34;https://gow.epsrc.ukri.org/NGBOViewGrant.aspx?GrantRef=EP/Y03516X/1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;announcement 2&lt;/a&gt; from EPSRC.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>[12/23, Paper &amp; Award] TorchOpt is accepted by JMLR and becomes a PyTorch Ecosystem Project.</title>
      <link>https://luomai.github.io/post/23-torchopt-jmlr/</link>
      <pubDate>Sun, 10 Dec 2023 13:35:37 +0100</pubDate>
      <guid>https://luomai.github.io/post/23-torchopt-jmlr/</guid>
      <description>&lt;p&gt;After 2 years incubation, 
&lt;a href=&#34;https://github.com/metaopt/torchopt&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;TorchOpt&lt;/a&gt; has become a PyTorch Ecosystem Project 
&lt;a href=&#34;https://medium.com/pytorch/introducing-torchopt-a-high-performance-differentiable-optimization-library-for-pytorch-37c4c0ef6ae1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Announcement&lt;/a&gt;. Its paper is also accepted by the JMLR in the open-source software track.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>[10/23, Award] Finalist for the Chancellor&#39;s Rising Star in Research.</title>
      <link>https://luomai.github.io/post/23-rising-star/</link>
      <pubDate>Fri, 10 Nov 2023 13:35:37 +0100</pubDate>
      <guid>https://luomai.github.io/post/23-rising-star/</guid>
      <description>&lt;p&gt;I am nominated by the school for a Chancellor&amp;rsquo;s Rising Star in Research Award, a prestigious honour for young faculty joined University of Edinburgh between 2018 and 2023. The nomination passes highly-selective panels at the school and the college level, and becomes a finalist for the university level.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>[09/23, Paper] GEAR, a new training system for bridging RL and LLMs, accepted to ICML 2023.</title>
      <link>https://luomai.github.io/post/23-gear-icml/</link>
      <pubDate>Sun, 10 Sep 2023 13:35:37 +0100</pubDate>
      <guid>https://luomai.github.io/post/23-gear-icml/</guid>
      <description>&lt;p&gt;Gear system to appear in ICML 2023.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>[07/22, Paper] Ekko, the first system unifying training and inference, accepted to OSDI 2022</title>
      <link>https://luomai.github.io/post/22-ekko-osdi/</link>
      <pubDate>Sun, 18 Dec 2022 13:35:37 +0100</pubDate>
      <guid>https://luomai.github.io/post/22-ekko-osdi/</guid>
      <description>&lt;p&gt;Our Paper &amp;ldquo;Ekko: A Large-Scale Deep Learning Recommender System with Low-Latency Model Update&amp;rdquo; is accepted by USENIX Symposium on Operating Systems Design and Implementation (OSDI) 2022. OSDI brings together professionals from academic and industrial backgrounds in what has become a premier forum for discussing the design, implementation, and implications of systems software. The symposium emphasizes innovative research as well as quantified or insightful experiences in systems design and implementation.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>MegBA library in ECCV 2022</title>
      <link>https://luomai.github.io/post/22-megba-eccv/</link>
      <pubDate>Sun, 10 Jul 2022 13:35:37 +0100</pubDate>
      <guid>https://luomai.github.io/post/22-megba-eccv/</guid>
      <description>&lt;p&gt;Our Paper &amp;ldquo;MegBA: A GPU-Based Distributed Library for Large-Scale Bundle Adjustment&amp;rdquo; is accepted by ECCV 2022.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Cameo system in NSDI 2021</title>
      <link>https://luomai.github.io/post/21-cameo-accept-news/</link>
      <pubDate>Mon, 05 Jul 2021 13:45:55 +0100</pubDate>
      <guid>https://luomai.github.io/post/21-cameo-accept-news/</guid>
      <description>&lt;p&gt;Our Paper &amp;ldquo;Move Fast and Meet Deadlines: Fine-grained Real-time Stream Processing with Cameo&amp;rdquo; is accepted by USENIX Symposium on Networked Systems Design and Implementation (NSDI) 2021.&lt;/p&gt;
&lt;p&gt;NSDI focuses on the design principles, implementation, and practical evaluation of networked and distributed systems. Its goal is to bring together researchers from across the networking and systems community to foster a broad approach to addressing overlapping research challenges.&lt;/p&gt;
&lt;p&gt;NSDI provides a high-quality forum for presenting results and discussing ideas that further the knowledge and understanding of the networked systems community as a whole, continue a significant research dialog, or push the architectural boundaries of network services.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>KungFu system in OSDI 2020</title>
      <link>https://luomai.github.io/post/20-kungfu-accept-news/</link>
      <pubDate>Tue, 18 Aug 2020 13:35:37 +0100</pubDate>
      <guid>https://luomai.github.io/post/20-kungfu-accept-news/</guid>
      <description>&lt;p&gt;Our Paper &amp;ldquo;KungFu: Making Training in Distributed Machine Learning Adaptive&amp;rdquo; is accepted by USENIX Symposium on Operating Systems Design and Implementation (OSDI) 2020.&lt;/p&gt;
&lt;p&gt;OSDI brings together professionals from academic and industrial backgrounds in what has become a premier forum for discussing the design, implementation, and implications of systems software. The symposium emphasizes innovative research as well as quantified or insightful experiences in systems design and implementation.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Invited talk in the AI systems workshop at SOSP 2019</title>
      <link>https://luomai.github.io/post/19-talk-sosp/</link>
      <pubDate>Tue, 05 Nov 2019 13:53:37 +0100</pubDate>
      <guid>https://luomai.github.io/post/19-talk-sosp/</guid>
      <description>&lt;p&gt;I am invited to give a talk: &amp;ldquo;Adaptive Distributed Training of Deep Learning Models&amp;rdquo; in the Workshop on AI Systems at ACM Symposium on Operating Systems Principles (SOSP) 2019.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
