<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Luo Mai</title>
    <link>https://luomai.github.io/</link>
      <atom:link href="https://luomai.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Luo Mai</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Fri, 26 Sep 2025 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://luomai.github.io/images/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url>
      <title>Luo Mai</title>
      <link>https://luomai.github.io/</link>
    </image>
    
    <item>
      <title>MoE-CAP: Benchmarking Cost, Accuracy and Performance of Sparse Mixture-of-Experts Systems</title>
      <link>https://luomai.github.io/publication/2025-neurips-moecap/</link>
      <pubDate>Fri, 26 Sep 2025 00:00:00 +0000</pubDate>
      <guid>https://luomai.github.io/publication/2025-neurips-moecap/</guid>
      <description></description>
    </item>
    
    <item>
      <title>[09/25, Paper] MoE-CAP accepted to NeurIPS 2025 (Dataset and Benchmark Track).</title>
      <link>https://luomai.github.io/post/25-moecap-neurips/</link>
      <pubDate>Thu, 25 Sep 2025 13:35:37 +0100</pubDate>
      <guid>https://luomai.github.io/post/25-moecap-neurips/</guid>
      <description></description>
    </item>
    
    <item>
      <title>[08/25, Grant] Win a prestigious award to build systems for powering AI4Math breakthroughs.</title>
      <link>https://luomai.github.io/post/25-ai4math/</link>
      <pubDate>Mon, 25 Aug 2025 13:35:37 +0100</pubDate>
      <guid>https://luomai.github.io/post/25-ai4math/</guid>
      <description>&lt;p&gt;Excited to share that our SketchPad project has been selected by the AI4Math fund to advance breakthroughs in mathematics! I’ll contribute my expertise in building large-scale, efficient, and reliable AI systems. The project is led by Wenda Li, together with Huajian Xin, Lawrence C. Paulson, and me.&lt;/p&gt;
&lt;p&gt;Thanks to this generous support, we will be hiring several fully funded PhD students and Postdoctoral researchers. If you are passionate about pushing AI to tackle high-impact mathematical problems and other game-changing scientific discovery challenges, we’d love to hear from you.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>[07/25, Paper] WaferLLM, the world fastest LLM inference system, accepted to OSDI 2025.</title>
      <link>https://luomai.github.io/post/25-waferllm-osdi/</link>
      <pubDate>Tue, 01 Jul 2025 13:35:37 +0100</pubDate>
      <guid>https://luomai.github.io/post/25-waferllm-osdi/</guid>
      <description>&lt;p&gt;Emerging AI accelerators increasingly adopt wafer-scale manufacturing technologies, integrating hundreds of thousands of AI cores in a mesh architecture with large distributed on-chip memory (tens of GB in total) and ultra-high on-chip memory bandwidth (tens of PB/s). However, current LLM inference systems, optimized for shared memory architectures like GPUs, fail to exploit these accelerators fully.&lt;/p&gt;
&lt;p&gt;We introduce WaferLLM, the first wafer-scale LLM inference system. WaferLLM is guided by a novel PLMR model (pronounced as &amp;ldquo;Plummer&amp;rdquo;) that captures the unique hardware characteristics of wafer-scale architectures. Leveraging this model, WaferLLM pioneers wafer-scale LLM parallelism, optimizing the utilization of hundreds of thousands of on-chip cores. It also introduces MeshGEMM and MeshGEMV, the first GEMM and GEMV implementations designed to scale effectively on wafer-scale accelerators.&lt;/p&gt;
&lt;p&gt;Evaluations show that WaferLLM achieves up to 200× higher accelerator utilization than state-of-the-art methods. Leveraging a wafer-scale accelerator (Cerebras WSE2), WaferLLM delivers GEMV operations 606× faster and 16× more energy-efficient than on an NVIDIA A100 GPU. For full LLM inference, WaferLLM achieves 10-20× speedups over A100 GPU clusters running SGLang and vLLM. These advantages are expected to grow as wafer-scale AI models, software, and hardware continue to mature. WaferLLM is open-sourced at: 
&lt;a href=&#34;https://github.com/MeshInfra/WaferLLM&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/MeshInfra/WaferLLM&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>WaferLLM: Large Language Model Inference at Wafer Scale</title>
      <link>https://luomai.github.io/publication/2025-osdi-waferllm/</link>
      <pubDate>Wed, 26 Mar 2025 00:00:00 +0000</pubDate>
      <guid>https://luomai.github.io/publication/2025-osdi-waferllm/</guid>
      <description></description>
    </item>
    
    <item>
      <title>[03/25, Achievement] Starting August 2025, I’ll be Reader (Associate Professor).</title>
      <link>https://luomai.github.io/post/25-reader-promotion/</link>
      <pubDate>Tue, 25 Mar 2025 13:35:37 +0100</pubDate>
      <guid>https://luomai.github.io/post/25-reader-promotion/</guid>
      <description>&lt;p&gt;After nearly five years at Edinburgh, I’m delighted to share that my promotion has been approved, and I will take up the role of Reader (Associate Professor) in August 2025. Sincere thanks to all my students and postdocs, my collaborators around the world, and my colleagues at the School of Informatics, EPCC, and the University for their support along the way.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>[10/24, Grant] Secured a prestigious ARIA grant with Imperial College &amp; Cambridge University.</title>
      <link>https://luomai.github.io/post/24-aria-award/</link>
      <pubDate>Sat, 26 Oct 2024 13:35:37 +0100</pubDate>
      <guid>https://luomai.github.io/post/24-aria-award/</guid>
      <description>&lt;p&gt;I am deeply honored to receive a prestigious grant from the Advanced Research + Invention Agency (ARIA) to develop a scalable and modular performance simulation framework for emerging AI models, systems, and hardware. This project will be co-led by Imperial (Aaron Zhao), Edinburgh (Luo Mai), and Cambridge (Robert Mullins), bringing together over 10 world-leading experts across the entire system stack. With this £4.5M funding, we will build a critical mass of talent and skills to incubate game-changing AI systems, enabling a 1000X improvement in AI&amp;rsquo;s efficiency.&lt;/p&gt;
&lt;p&gt;See the 
&lt;a href=&#34;https://www.aria.org.uk/scaling-compute/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;official announcement&lt;/a&gt; from ARIA.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>[10/24, Paper] Tenplex, the first elastic LLM system, accepted to SOSP 2024.</title>
      <link>https://luomai.github.io/post/24-tenplex-sosp/</link>
      <pubDate>Fri, 25 Oct 2024 13:35:37 +0100</pubDate>
      <guid>https://luomai.github.io/post/24-tenplex-sosp/</guid>
      <description>&lt;p&gt;Tenplex is a new system enabling elastic training of LLMs with advanced multi-dimensional parallelism. The Tenplex paper has been accepted for presentation at the 30th Symposium on Operating Systems Principles (SOSP’24). We are currently preparing to open-source the project. Stay tuned.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ServerlessLLM</title>
      <link>https://luomai.github.io/project/serverlessllm/</link>
      <pubDate>Tue, 01 Oct 2024 00:00:00 +0000</pubDate>
      <guid>https://luomai.github.io/project/serverlessllm/</guid>
      <description>&lt;p&gt;ServerlessLLM is an open-source framework dedicated to making custom LLM deployment easy, fast, and affordable. As models grow in size and complexity, deploying them on distributed GPUs has become increasingly costly and technically challenging, limiting the benefits of custom LLM deployment to only a select few. ServerlessLLM tackles these challenges by a full-stack, LLM-centric serverless system design, integrating multiple LLM-optimized layers—from checkpoint formats and inference runtimes to the storage layer and cluster scheduler.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>[07/24, Student Achievement] Congrats to Yao Fu on Winning 2024 Rising Star in ML &amp; Systems.</title>
      <link>https://luomai.github.io/post/24-risingstar-yao/</link>
      <pubDate>Sat, 10 Aug 2024 13:35:37 +0100</pubDate>
      <guid>https://luomai.github.io/post/24-risingstar-yao/</guid>
      <description>&lt;p&gt;This year’s Rising Stars cohort includes 41 junior researchers from 33 institutions globally. Selected from over 170 applicants, Yao is the sole recipient from the UK and one of three from Europe, the other two European students coming from ETH, Zurich.&lt;/p&gt;
&lt;p&gt;The 2024 Rising Start workshop was held in the NVIDIA HQ in Santa Clara, CA in July, where Yao presented his research.&lt;/p&gt;
&lt;p&gt;Yao is a third-year PhD student in Computer Science at The University of Edinburgh, supervised by Dr Luo Mai. His research lies at the intersection of machine learning and systems, focusing on performance and affordability. Recently, his work has centred on efficient serving systems for large language models, leading to two main projects: ServerlessLLM and the MoESys Leaderboard.&lt;/p&gt;
&lt;p&gt;See the coverage from 
&lt;a href=&#34;https://informatics.ed.ac.uk/news-events/news/latest-news/informatics-student-among-machine-learning-and-systems-rising-stars&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;our school&lt;/a&gt; and the 
&lt;a href=&#34;https://mlcommons.org/2024/06/2024-mlc-rising-stars/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;annoucement from MLCommon&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tenplex: Dynamic Parallelism for Deep Learning using Parallelizable Tensor Collections</title>
      <link>https://luomai.github.io/publication/2024-sosp-tenplex/</link>
      <pubDate>Fri, 26 Jul 2024 00:00:00 +0000</pubDate>
      <guid>https://luomai.github.io/publication/2024-sosp-tenplex/</guid>
      <description></description>
    </item>
    
    <item>
      <title>[07/24, Paper] ServerlessLLM, the first serverless LLM system, accepted to OSDI 2024.</title>
      <link>https://luomai.github.io/post/24-serverlessllm-osdi/</link>
      <pubDate>Wed, 10 Jul 2024 13:35:37 +0100</pubDate>
      <guid>https://luomai.github.io/post/24-serverlessllm-osdi/</guid>
      <description>&lt;p&gt;ServerlessLLM is a new system enabling cost-effective serverless inference for LLMs by implementing a scalable and high-performance “checkpoint storage layer” on GPU servers. It achieves this through an innovative LLM checkpoint format, a multi-tier checkpoint loading subsystem, an efficient live migration algorithm, and a locality-friendly GPU serverless architecture.&lt;/p&gt;
&lt;p&gt;The ServerlessLLM paper has been accepted to the top systems conference, OSDI’24, and we are preparing to open-source the project. Stay tuned.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Learning high-frequency functions made easy with sinusoidal positional encoding</title>
      <link>https://luomai.github.io/publication/2024-icml-spe/</link>
      <pubDate>Sat, 01 Jun 2024 00:00:00 +0000</pubDate>
      <guid>https://luomai.github.io/publication/2024-icml-spe/</guid>
      <description></description>
    </item>
    
    <item>
      <title>[05/24, Award] Win a Microsoft Research Startrack Scholar Award.</title>
      <link>https://luomai.github.io/post/24-startrack-microsoft/</link>
      <pubDate>Fri, 10 May 2024 13:35:37 +0100</pubDate>
      <guid>https://luomai.github.io/post/24-startrack-microsoft/</guid>
      <description>&lt;p&gt;I am thrilled to receive the Microsoft Research Startrack Scholar Award, a prestigious honor for young faculty in the field of computer science.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ServerlessLLM: Low-Latency Serverless Inference for Large Language Models</title>
      <link>https://luomai.github.io/publication/2024-osdi-serverlessllm/</link>
      <pubDate>Fri, 26 Jan 2024 00:00:00 +0000</pubDate>
      <guid>https://luomai.github.io/publication/2024-osdi-serverlessllm/</guid>
      <description></description>
    </item>
    
    <item>
      <title>[03/24, Grant] Secured funds from EPSRC and industry partners to build a CDT for ML Systems.</title>
      <link>https://luomai.github.io/post/24-mlsyscdt-award/</link>
      <pubDate>Sat, 20 Jan 2024 13:35:37 +0100</pubDate>
      <guid>https://luomai.github.io/post/24-mlsyscdt-award/</guid>
      <description>&lt;p&gt;I am deeply thrilled to have secured an EPSRC grant (£8.7M) matched with £9.2M from industry partners to establish a Centre for Doctoral Training (CDT) in ML Systems. This grant is co-led by Amos Storkey (PI), Mike O&amp;rsquo;Boyle (co-I), Ajitha Rajan (co-I), and myself (co-I). With this CDT, we are one of the few places in the world capable of creating the critical mass of talent, skills, and resources needed to incubate game-changing technologies for ML systems.&lt;/p&gt;
&lt;p&gt;Through this CDT, PhD students will have the opportunity to build expertise across the entire AI system stack—spanning models, algorithms, software, hardware, and large-scale clusters. The CDT is supported by UK EPSRC and over 20 industry partners, with that number still growing.&lt;/p&gt;
&lt;p&gt;See the 
&lt;a href=&#34;https://www.ukri.org/what-we-do/developing-people-and-skills/epsrc/studentships/centres-for-doctoral-training/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;announcement 1&lt;/a&gt; and 
&lt;a href=&#34;https://gow.epsrc.ukri.org/NGBOViewGrant.aspx?GrantRef=EP/Y03516X/1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;announcement 2&lt;/a&gt; from EPSRC.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>TorchOpt: An Efficient Library for Differentiable Optimization</title>
      <link>https://luomai.github.io/publication/2023-jmlr-torchopt/</link>
      <pubDate>Fri, 15 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://luomai.github.io/publication/2023-jmlr-torchopt/</guid>
      <description></description>
    </item>
    
    <item>
      <title>[12/23, Paper &amp; Award] TorchOpt is accepted by JMLR and becomes a PyTorch Ecosystem Project.</title>
      <link>https://luomai.github.io/post/23-torchopt-jmlr/</link>
      <pubDate>Sun, 10 Dec 2023 13:35:37 +0100</pubDate>
      <guid>https://luomai.github.io/post/23-torchopt-jmlr/</guid>
      <description>&lt;p&gt;After 2 years incubation, 
&lt;a href=&#34;https://github.com/metaopt/torchopt&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;TorchOpt&lt;/a&gt; has become a PyTorch Ecosystem Project 
&lt;a href=&#34;https://medium.com/pytorch/introducing-torchopt-a-high-performance-differentiable-optimization-library-for-pytorch-37c4c0ef6ae1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Announcement&lt;/a&gt;. Its paper is also accepted by the JMLR in the open-source software track.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>[10/23, Award] Finalist for the Chancellor&#39;s Rising Star in Research.</title>
      <link>https://luomai.github.io/post/23-rising-star/</link>
      <pubDate>Fri, 10 Nov 2023 13:35:37 +0100</pubDate>
      <guid>https://luomai.github.io/post/23-rising-star/</guid>
      <description>&lt;p&gt;I am nominated by the school for a Chancellor&amp;rsquo;s Rising Star in Research Award, a prestigious honour for young faculty joined University of Edinburgh between 2018 and 2023. The nomination passes highly-selective panels at the school and the college level, and becomes a finalist for the university level.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>[09/23, Paper] GEAR, a new training system for bridging RL and LLMs, accepted to ICML 2023.</title>
      <link>https://luomai.github.io/post/23-gear-icml/</link>
      <pubDate>Sun, 10 Sep 2023 13:35:37 +0100</pubDate>
      <guid>https://luomai.github.io/post/23-gear-icml/</guid>
      <description>&lt;p&gt;Gear system to appear in ICML 2023.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>GEAR: A GPU-Centric Experience Replay System for Large Reinforcement Learning Models</title>
      <link>https://luomai.github.io/publication/2023-icml-gear/</link>
      <pubDate>Mon, 01 May 2023 00:00:00 +0000</pubDate>
      <guid>https://luomai.github.io/publication/2023-icml-gear/</guid>
      <description></description>
    </item>
    
    <item>
      <title>[07/22, Paper] Ekko, the first system unifying training and inference, accepted to OSDI 2022</title>
      <link>https://luomai.github.io/post/22-ekko-osdi/</link>
      <pubDate>Sun, 18 Dec 2022 13:35:37 +0100</pubDate>
      <guid>https://luomai.github.io/post/22-ekko-osdi/</guid>
      <description>&lt;p&gt;Our Paper &amp;ldquo;Ekko: A Large-Scale Deep Learning Recommender System with Low-Latency Model Update&amp;rdquo; is accepted by USENIX Symposium on Operating Systems Design and Implementation (OSDI) 2022. OSDI brings together professionals from academic and industrial backgrounds in what has become a premier forum for discussing the design, implementation, and implications of systems software. The symposium emphasizes innovative research as well as quantified or insightful experiences in systems design and implementation.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Ekko: A Large-Scale Deep Learning Recommender System with Low-Latency Model Update</title>
      <link>https://luomai.github.io/publication/2022-osdi-ekko/</link>
      <pubDate>Tue, 01 Nov 2022 00:00:00 +0000</pubDate>
      <guid>https://luomai.github.io/publication/2022-osdi-ekko/</guid>
      <description></description>
    </item>
    
    <item>
      <title>MegBA</title>
      <link>https://luomai.github.io/project/megba/</link>
      <pubDate>Mon, 31 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://luomai.github.io/project/megba/</guid>
      <description>&lt;p&gt;MegBA is a fast and distributed library for large-scale Bundle Adjustment (BA). MegBA has a novel end-to-end vectorised BA algorithm which can fully exploit the massive parallel cores on GPUs, thus speeding up the entire BA computation. It also has a novel distributed BA algorithm that can automatically partition BA problems, and solve BA sub-problems using distributed GPUs. The GPUs synchronise intermediate solving state using network-efficient collective communication, and the synchronisation is designed to minimise communication cost. MegBA has a memory-efficient GPU runtime and it exposes g2o-compatible APIs. Experiments show that MegBA can out-perform state-of-the-art BA libraries (i.e., Ceres and DeepLM) by ~50x and ~5x respectively, in public large-scale BA benchmarks.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>TorchOpt</title>
      <link>https://luomai.github.io/project/torchopt/</link>
      <pubDate>Mon, 31 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://luomai.github.io/project/torchopt/</guid>
      <description>&lt;p&gt;TorchOpt is an efficient library for differentiable optimization built upon PyTorch. TorchOpt is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Comprehensive&lt;/strong&gt;: TorchOpt provides three differentiation modes - explicit differentiation, implicit differentiation, and zero-order differentiation for handling different differentiable optimization situations.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Flexible&lt;/strong&gt;: TorchOpt provides both functional and objective-oriented API for users&amp;rsquo; different preferences. Users can implement differentiable optimization in JAX-like or PyTorch-like style.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Efficient&lt;/strong&gt;: TorchOpt provides (1) CPU/GPU acceleration differentiable optimizer (2) RPC-based distributed training framework (3) Fast Tree Operations, to largely increase the training efficiency for bi-level optimization problems.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Beyond differentiable optimization, TorchOpt can also be regarded as a functional optimizer that enables JAX-like composable functional optimizer for PyTorch. With TorchOpt, users can easily conduct neural network optimization in PyTorch with a functional style optimizer, similar to Optax in JAX.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A Theoretical Understanding of Gradient Bias in Meta-Reinforcement Learning</title>
      <link>https://luomai.github.io/publication/2022-neurips-settling/</link>
      <pubDate>Mon, 15 Aug 2022 00:00:00 +0000</pubDate>
      <guid>https://luomai.github.io/publication/2022-neurips-settling/</guid>
      <description></description>
    </item>
    
    <item>
      <title>MegBA: A GPU-Based Distributed Library for Large-Scale Bundle Adjustment</title>
      <link>https://luomai.github.io/publication/2022-eccv-megba/</link>
      <pubDate>Wed, 10 Aug 2022 00:00:00 +0000</pubDate>
      <guid>https://luomai.github.io/publication/2022-eccv-megba/</guid>
      <description></description>
    </item>
    
    <item>
      <title>MegBA library in ECCV 2022</title>
      <link>https://luomai.github.io/post/22-megba-eccv/</link>
      <pubDate>Sun, 10 Jul 2022 13:35:37 +0100</pubDate>
      <guid>https://luomai.github.io/post/22-megba-eccv/</guid>
      <description>&lt;p&gt;Our Paper &amp;ldquo;MegBA: A GPU-Based Distributed Library for Large-Scale Bundle Adjustment&amp;rdquo; is accepted by ECCV 2022.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Move Fast and Meet Deadlines: Fine-grained Real-time Stream Processing with Cameo</title>
      <link>https://luomai.github.io/publication/2021-nsdi-cameo/</link>
      <pubDate>Wed, 01 Dec 2021 00:00:00 +0000</pubDate>
      <guid>https://luomai.github.io/publication/2021-nsdi-cameo/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Quiver</title>
      <link>https://luomai.github.io/project/quiver/</link>
      <pubDate>Sun, 31 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://luomai.github.io/project/quiver/</guid>
      <description>&lt;p&gt;The primary motivation for the Quiver project is to make it easy to take a PyG script and scale it across many GPUs and CPUs. A typical scenario is: Quiver users can leverage the high-level APIs and rich examples of PyG to design graph learning algorithms, and then use Quiver to scale PyG algorithms to run at large scale. To make such scaling efficient, Quiver has several features:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;High performance&lt;/strong&gt;: Quiver enables GPUs to be efficiently used in accelerating performance-critical graph learning tasks: graph sampling, feature collection and data-parallel training. Quiver thus can out-perform PyG and DGL even with a single GPU, especially when processing large-scale datasets and models.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;High scalability&lt;/strong&gt;: Quiver can achieve (super) linear scalability in distributed graph learning. This is contributed by Quiver&amp;rsquo;s novel communication-efficient data/processor management techniques and effective usage of fast networking technologies (e.g., NVLink and RDMA).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Easy to use&lt;/strong&gt;: Quiver requires only a few new lines of code in existing PyG programs and it has no external heavy dependency. Quiver is thus easy to be adopted by PyG users and integrated into production clusters.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Efficient Reinforcement Learning Development with RLzoo</title>
      <link>https://luomai.github.io/publication/2021-multimedia-rlzoo/</link>
      <pubDate>Sun, 15 Aug 2021 00:00:00 +0000</pubDate>
      <guid>https://luomai.github.io/publication/2021-multimedia-rlzoo/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Fast and Flexible Human Pose Estimation with HyperPose</title>
      <link>https://luomai.github.io/publication/2021-multimedia-hyperpose/</link>
      <pubDate>Sun, 15 Aug 2021 00:00:00 +0000</pubDate>
      <guid>https://luomai.github.io/publication/2021-multimedia-hyperpose/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Cameo system in NSDI 2021</title>
      <link>https://luomai.github.io/post/21-cameo-accept-news/</link>
      <pubDate>Mon, 05 Jul 2021 13:45:55 +0100</pubDate>
      <guid>https://luomai.github.io/post/21-cameo-accept-news/</guid>
      <description>&lt;p&gt;Our Paper &amp;ldquo;Move Fast and Meet Deadlines: Fine-grained Real-time Stream Processing with Cameo&amp;rdquo; is accepted by USENIX Symposium on Networked Systems Design and Implementation (NSDI) 2021.&lt;/p&gt;
&lt;p&gt;NSDI focuses on the design principles, implementation, and practical evaluation of networked and distributed systems. Its goal is to bring together researchers from across the networking and systems community to foster a broad approach to addressing overlapping research challenges.&lt;/p&gt;
&lt;p&gt;NSDI provides a high-quality forum for presenting results and discussing ideas that further the knowledge and understanding of the networked systems community as a whole, continue a significant research dialog, or push the architectural boundaries of network services.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>KungFu: Making Training in Distributed Machine Learning Adaptive</title>
      <link>https://luomai.github.io/publication/2020-osdi-kungfu/</link>
      <pubDate>Tue, 01 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://luomai.github.io/publication/2020-osdi-kungfu/</guid>
      <description></description>
    </item>
    
    <item>
      <title>KungFu system in OSDI 2020</title>
      <link>https://luomai.github.io/post/20-kungfu-accept-news/</link>
      <pubDate>Tue, 18 Aug 2020 13:35:37 +0100</pubDate>
      <guid>https://luomai.github.io/post/20-kungfu-accept-news/</guid>
      <description>&lt;p&gt;Our Paper &amp;ldquo;KungFu: Making Training in Distributed Machine Learning Adaptive&amp;rdquo; is accepted by USENIX Symposium on Operating Systems Design and Implementation (OSDI) 2020.&lt;/p&gt;
&lt;p&gt;OSDI brings together professionals from academic and industrial backgrounds in what has become a premier forum for discussing the design, implementation, and implications of systems software. The symposium emphasizes innovative research as well as quantified or insightful experiences in systems design and implementation.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>KungFu</title>
      <link>https://luomai.github.io/project/kungfu/</link>
      <pubDate>Mon, 01 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://luomai.github.io/project/kungfu/</guid>
      <description>&lt;p&gt;Today&amp;rsquo;s machine learning systems must cope with growing complex models and increasingly complicated deployment environments, making them difficult to constantly deliver high performance with an empirical configuration. To address this, KungFu enables machine learning users to realise adaptive distributed training policies using high-level training monitoring and control APIs. KungFu has a fast and scalable runtime which can automatically scale out policy execution onto distributed GPU servers. Large-scale cluster experiments show that KungFu not only enables real-world adaptive training use cases, but also out-performs state-of-the-art distributed training systems including Horovod and Parameters Servers.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Spotnik: Designing Distributed Machine Learning for Transient Cloud Resources</title>
      <link>https://luomai.github.io/publication/2020-hotcloud-spotnik/</link>
      <pubDate>Mon, 01 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://luomai.github.io/publication/2020-hotcloud-spotnik/</guid>
      <description></description>
    </item>
    
    <item>
      <title>TensorLayer</title>
      <link>https://luomai.github.io/project/tensorlayer/</link>
      <pubDate>Fri, 01 May 2020 00:00:00 +0000</pubDate>
      <guid>https://luomai.github.io/project/tensorlayer/</guid>
      <description>&lt;p&gt;TensorLayer is a popular open-source TensorFlow-based deep learning and reinforcement library. It provides rich neural layers, pre-trained neural networks, data processing, model management and distributed training to facilitate the development of a wide spectrum of deep learning algorithms. TensorLayer has a transparent and flexible programming model, and thus suitable to customise deep neural networks for deployment and research purposes. It exhibits superior performance due to the low-cost integration with the TensorFlow backend. TensorLayer is easy to learn by providing massive tutorials, examples and real-world application codes. Since open-sourced in 2017, it has attracted more than 300,000 downloads, 6000 stars on Github, and 90 contributors around the world. TensorLayer is recently awarded the 2017 best open-source software by the prestigious ACM multimedia community.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>HyperPose</title>
      <link>https://luomai.github.io/project/hyperpose/</link>
      <pubDate>Wed, 01 Apr 2020 00:00:00 +0000</pubDate>
      <guid>https://luomai.github.io/project/hyperpose/</guid>
      <description>&lt;p&gt;Estimating human pose is a core task in many multimedia applications.
To fully achieve its promise,
users often need to customise pose estimation systems for
best possible accuracy, and optimise the systems so that they can achieve real-time processing
of high-resolution video streams.
To meet these needs, we introduce HyperPose, a library for building pose estimation systems.
HyperPose provides a large collection of high-level APIs to help
users develop pose estimation algorithms that can achieve high
accuracy in the wild.
HyperPose further provides a high-performance algorithm execution engine.
This engine has a high-performance dataflow for
executing pose estimation algorithms. It dynamically
dispatches dataflow operators onto CPUs/GPUs, which maximises
hardware efficiency, thus achieving real-time processing. Evaluation result show that HyperPose allows users
to declare many useful pose estimation algorithms. It also
out-performs the performance of state-of-the-art pose estimation systems by up to 3.1x.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Invited talk in the AI systems workshop at SOSP 2019</title>
      <link>https://luomai.github.io/post/19-talk-sosp/</link>
      <pubDate>Tue, 05 Nov 2019 13:53:37 +0100</pubDate>
      <guid>https://luomai.github.io/post/19-talk-sosp/</guid>
      <description>&lt;p&gt;I am invited to give a talk: &amp;ldquo;Adaptive Distributed Training of Deep Learning Models&amp;rdquo; in the Workshop on AI Systems at ACM Symposium on Operating Systems Principles (SOSP) 2019.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>SOSP AI Systems Workshop</title>
      <link>https://luomai.github.io/talk/2019-kungfu-sosp/</link>
      <pubDate>Sun, 27 Oct 2019 13:00:00 +0000</pubDate>
      <guid>https://luomai.github.io/talk/2019-kungfu-sosp/</guid>
      <description></description>
    </item>
    
    <item>
      <title>CrossBow: Scaling Deep Learning with Small Batch Sizes on Multi-GPU Servers</title>
      <link>https://luomai.github.io/publication/2019-vldb-crossbow/</link>
      <pubDate>Sun, 01 Sep 2019 00:00:00 +0000</pubDate>
      <guid>https://luomai.github.io/publication/2019-vldb-crossbow/</guid>
      <description></description>
    </item>
    
    <item>
      <title>RLzoo</title>
      <link>https://luomai.github.io/project/rlzoo/</link>
      <pubDate>Sun, 01 Sep 2019 00:00:00 +0000</pubDate>
      <guid>https://luomai.github.io/project/rlzoo/</guid>
      <description>&lt;p&gt;Deep Reinforcement Learning (DRL) has become the foundation of many multimedia applications. To fully achieve its promise, multi-media users are looking for a library that allows them to efficiently design and test DRL agents and integrate the agents into their ap-plications. In this project, we introduce RLzoo, a novel DRL library that makes it easy to design, test and deploy DRL agents. RLzoo has high-level expressive APIs which enable its users to efficiently develop DRL agents. RLzoo users can leverage an automatic agent construction algorithm to seamlessly adopt custom agent modules,e.g., custom neural networks, which is the key for tuning agents for achieving the best possible performance. RLzoo users can access a large number of pre-implemented DRL environments and algorithms, making it a comprehensive DRL platform. On this platform,users can further easily manage and tune DRL agents through an interactive training terminal. Evaluation results show that: com-pared to existing DRL libraries, RLzoo not only achieves a high degree of abstraction in its API design. It also provides numerous useful DRL algorithms and environments which are not available in other libraries.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Taming Hyper-parameters in Deep Learning Systems</title>
      <link>https://luomai.github.io/publication/2019-osr-review/</link>
      <pubDate>Mon, 01 Jul 2019 00:00:00 +0000</pubDate>
      <guid>https://luomai.github.io/publication/2019-osr-review/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Google Developer Festival</title>
      <link>https://luomai.github.io/talk/2018-google-devfest/</link>
      <pubDate>Sat, 01 Dec 2018 13:00:00 +0000</pubDate>
      <guid>https://luomai.github.io/talk/2018-google-devfest/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Chi: A Scalable and Programmable Control Plane for Distributed Stream Processing Systems</title>
      <link>https://luomai.github.io/publication/2018-vldb-chi/</link>
      <pubDate>Sun, 01 Jul 2018 00:00:00 +0000</pubDate>
      <guid>https://luomai.github.io/publication/2018-vldb-chi/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Emu: Rapid Prototyping of Networking Services</title>
      <link>https://luomai.github.io/publication/2017-atc-emu/</link>
      <pubDate>Tue, 01 Aug 2017 00:00:00 +0000</pubDate>
      <guid>https://luomai.github.io/publication/2017-atc-emu/</guid>
      <description></description>
    </item>
    
    <item>
      <title>TensorLayer: A Versatile Library for Efficient Deep Learning Development</title>
      <link>https://luomai.github.io/publication/2017-multimedia-tensorlayer/</link>
      <pubDate>Sat, 01 Jul 2017 00:00:00 +0000</pubDate>
      <guid>https://luomai.github.io/publication/2017-multimedia-tensorlayer/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Flick: Developing and Running Application-specific Network Services </title>
      <link>https://luomai.github.io/publication/2016-atc-flick/</link>
      <pubDate>Mon, 01 Aug 2016 00:00:00 +0000</pubDate>
      <guid>https://luomai.github.io/publication/2016-atc-flick/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Towards a Network Marketplace in a Cloud</title>
      <link>https://luomai.github.io/publication/2016-hotcloud-netex/</link>
      <pubDate>Fri, 01 Jul 2016 00:00:00 +0000</pubDate>
      <guid>https://luomai.github.io/publication/2016-hotcloud-netex/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Optimizing Network Performance in Distributed Machine Learning</title>
      <link>https://luomai.github.io/publication/2015-hotcloud-mlnet/</link>
      <pubDate>Wed, 01 Jul 2015 00:00:00 +0000</pubDate>
      <guid>https://luomai.github.io/publication/2015-hotcloud-mlnet/</guid>
      <description></description>
    </item>
    
    <item>
      <title>NetAgg: Using Middleboxes for Application-specific On-path Aggregation in Data Centres</title>
      <link>https://luomai.github.io/publication/2014-conext-netagg/</link>
      <pubDate>Mon, 01 Dec 2014 00:00:00 +0000</pubDate>
      <guid>https://luomai.github.io/publication/2014-conext-netagg/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Exploiting Time-malleability in Cloud-based Batch Processing Systems</title>
      <link>https://luomai.github.io/publication/2013-ladis-mai/</link>
      <pubDate>Sun, 01 Dec 2013 00:00:00 +0000</pubDate>
      <guid>https://luomai.github.io/publication/2013-ladis-mai/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Load Balanced Rendezvous Data Collection in Wireless Sensor Networks</title>
      <link>https://luomai.github.io/publication/2011-mass-mai/</link>
      <pubDate>Thu, 01 Dec 2011 00:00:00 +0000</pubDate>
      <guid>https://luomai.github.io/publication/2011-mass-mai/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
